# Quiz

### Q1: What is the main goal of Natural Language Processing (NLP)?

- A. To develop programming languages that resemble natural languages
- B. To enable computers to understand, interpret, and generate human language
- C. To create artificial languages for computer-to-computer communication
- D. To translate computer languages into human-readable formats

### Q2: Which of the following is NOT a challenge in NLP mentioned in the lesson?

- A. Part-of-Speech Tagging
- B. Word Sense Disambiguation
- C. Neural Machine Translation
- D. Imperfect or Irregular Input

### Q3: What is tokenization in NLP?

- A. The process of encrypting text for secure transmission
- B. The process of breaking down text into smaller units called tokens
- C. The process of categorizing text into different languages
- D. The process of removing stop words from text

### Q4: What is the main difference between stemming and lemmatization?

- A. Stemming is faster but less accurate, while lemmatization considers the context and produces actual words
- B. Stemming works only on nouns, while lemmatization works on all parts of speech
- C. Stemming is primarily used for non-English languages, while lemmatization is for English
- D. Stemming removes prefixes, while lemmatization removes suffixes

### Q5: In the context of language modeling, what does an n-gram refer to?

- A. A measurement unit for text similarity
- B. A contiguous sequence of n items from a given sample of text
- C. A neural network with n hidden layers
- D. A grammar rule applied to n consecutive sentences

### Q6: What is TF-IDF used for in NLP?

- A. Text formatting and indexing document formatting
- B. To evaluate the importance of a word to a document in a collection or corpus
- C. To translate foreign languages into document format
- D. To identify duplicate files in document repositories

### Q7: What is cosine similarity in the context of NLP?

- A. A measure of the angle between two documents in vector space
- B. A measure of the physical proximity of words in a text
- C. A technique to compress large text documents
- D. A method to identify cosine patterns in text data

### Q8: Which of the following is an advantage of Word2Vec over traditional one-hot encoded vectors?

- A. Word2Vec requires less computational power
- B. Word2Vec produces sparse vectors that are easier to process
- C. Word2Vec captures semantic relationships between words
- D. Word2Vec works exclusively with English text

### Q9: What is a key feature of FastText that differentiates it from Word2Vec?

- A. FastText is significantly faster to train
- B. FastText considers subword information
- C. FastText can only be used for classification tasks
- D. FastText requires labeled data for training

### Q10: Why is Naive Bayes classifier considered "naive"?

- A. Because it makes a strong assumption of conditional independence between every pair of features
- B. Because it's a simplified version of a more complex algorithm
- C. Because it was one of the first machine learning algorithms developed
- D. Because it can only handle simple classification tasks
