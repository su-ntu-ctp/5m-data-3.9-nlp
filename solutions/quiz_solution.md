# Quiz Solution

### Q1: What is the main goal of Natural Language Processing (NLP)?

- A. To develop programming languages that resemble natural languages
- **B. To enable computers to understand, interpret, and generate human language**
- C. To create artificial languages for computer-to-computer communication
- D. To translate computer languages into human-readable formats

### Q2: Which of the following is NOT a challenge in NLP mentioned in the lesson?

- A. Part-of-Speech Tagging
- B. Word Sense Disambiguation
- **C. Neural Machine Translation**
- D. Imperfect or Irregular Input

### Q3: What is tokenization in NLP?

- A. The process of encrypting text for secure transmission
- **B. The process of breaking down text into smaller units called tokens**
- C. The process of categorizing text into different languages
- D. The process of removing stop words from text

### Q4: What is the main difference between stemming and lemmatization?

- **A. Stemming is faster but less accurate, while lemmatization considers the context and produces actual words**
- B. Stemming works only on nouns, while lemmatization works on all parts of speech
- C. Stemming is primarily used for non-English languages, while lemmatization is for English
- D. Stemming removes prefixes, while lemmatization removes suffixes

### Q5: In the context of language modeling, what does an n-gram refer to?

- A. A measurement unit for text similarity
- **B. A contiguous sequence of n items from a given sample of text**
- C. A neural network with n hidden layers
- D. A grammar rule applied to n consecutive sentences

### Q6: What is TF-IDF used for in NLP?

- A. Text formatting and indexing document formatting
- **B. To evaluate the importance of a word to a document in a collection or corpus**
- C. To translate foreign languages into document format
- D. To identify duplicate files in document repositories

### Q7: What is cosine similarity in the context of NLP?

- **A. A measure of the angle between two documents in vector space**
- B. A measure of the physical proximity of words in a text
- C. A technique to compress large text documents
- D. A method to identify cosine patterns in text data

### Q8: Which of the following is an advantage of Word2Vec over traditional one-hot encoded vectors?

- A. Word2Vec requires less computational power
- B. Word2Vec produces sparse vectors that are easier to process
- **C. Word2Vec captures semantic relationships between words**
- D. Word2Vec works exclusively with English text

### Q9: What is a key feature of FastText that differentiates it from Word2Vec?

- A. FastText is significantly faster to train
- **B. FastText considers subword information**
- C. FastText can only be used for classification tasks
- D. FastText requires labeled data for training

### Q10: Why is Naive Bayes classifier considered "naive"?

- **A. Because it makes a strong assumption of conditional independence between every pair of features**
- B. Because it's a simplified version of a more complex algorithm
- C. Because it was one of the first machine learning algorithms developed
- D. Because it can only handle simple classification tasks
